Machine Learning Pre-Processing: SCALING, STANDARDIZING, NORMALIZING

z-score :
Simply put, a z-score is the number of standard deviations from the mean a data point is. But more technically it's a measure of how many standard deviations below or above the population mean a raw score is. A z-score is also known as a standard score and it can be placed on a normal distribution curve.

Normalization/standardization are designed to achieve a similar goal, which is to create features that have similar ranges to each other and widely used in data analysis to help the programmer to get some clue out of the raw data.

Question: 
I'm told that I need to normalize features before modeling with KNN. What's the difference between scaling to 0 and 1, taking a unit norm, or z score? I've been taught all of these to "normalize" data, but don't know how or why each apply.

Answer:
RESCALING attribute data to values to scale the range in [0, 1] or [-1, 1] is useful for the optimization algorithms, such as gradient descent, that are used within machine learning algorithms that weight inputs (e.g. regression and neural networks). Rescaling is also used for algorithms that use distance measurements for example K-Nearest-Neighbors (KNN). Rescaling like this is sometimes called "normalization". MinMaxScaler class in python skikit-learn does this.

STANDARDIZING attribute data assumes a Gaussian distribution of input features and "standardizes" to a mean of 0 and a standard deviation of 1. This works better with linear regression, logistic regression and linear discriminate analysis. Python StandardScaler class in scikit-learn works for this.

NORMALIZING attribute data is used to rescale components of a feature vector to have the complete vector length of 1. This usually means dividing each component of the feature vector by the Euclidiean length of the vector but can also be Manhattan or other distance measurements. This pre-processing rescaling method is useful for sparse attribute features and algorithms using distance to learn such as KNN.

If you have outliers in your data set, normalizing your data will certainly scale the “normal” data to a very small interval. And generally, most of data sets have outliers. When using standardization, your new data aren’t bounded

Further Details:
In statistics, the standard score is the signed number of standard deviations by which an observation or datum is above the mean. A standard score is a derived score, used for making norm-referenced interpretations, for which the mean and standard deviation are selected to simplify interpretations

A positive standard score indicates a datum above the mean, while a negative standard score indicates a datum below the mean. It is a dimensionless quantity obtained by subtracting the population mean from an individual raw score and then dividing the difference by the population standard deviation. This conversion process is called standardizing or normalizing (however, "normalizing" can refer to many types of ratios; see normalization for more).

Scaling = scaling is changing numbers to the scales such as 0 to 100. It is personal choice. but for data science we mostly use standardization. normalization is generally scaling from 0 to 1. 