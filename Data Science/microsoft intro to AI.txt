Machine Learning
	supervised 
	unsupervised
	
Supervised Learning
	Regression = type of supervised learning. when we need to predict numerical (non-discrete) value use regression.
	
	feature is input; label is output
	A feature is one column of the data in your input set. For instance, if you're trying to predict the type of pet someone will choose, your input features might include age, home region, family income, etc. The label is the final choice, such as dog, fish, iguana, rock, etc.
	Once you've trained your model, you will give it sets of new input containing those features; it will return the predicted "label" (pet type) for that person.
	
	Example = One row of a data set. An example contains one or more features and possibly a label.
	Squred Loss = the squares of the difference between a model's predicted value for a labeled example and the actual value of the label. 
	Mean Squred Loss = The average squared loss per example. MSE is calculated by dividing the squared loss by the number of examples.
	Root Mean Squred Error = The square root of the Mean Squared Error.
	
	
	model = The representation of what an ML system has learned from the training data.
	ways to measure error in model = root mean square error, mean absolute error
	
	bell shape histogram
	disease score normalization
	different algorithms used in data science
	Inverser Document frequency 
	median
	sobel edge detection algorithm
	harris corner detection algorithm
	loss function = determines major difference between lot of machine learning methods.
	statistical learning theory
	principal of ockham razor = among hypothesis that predicts equally well we should choose the one with fewest assumptions.
								best models are the simple moedels which fits data well.
								we need to balance between accuracy and simplicity.
	cursive dimensionality = cursive dimensionality is that we tend to overfit when we have lot of features and not as much data
	Regularisation =
	logistic regression = find the weights that minimizes the sum of training loss. It just chooses the coefficients (those betas ) to minimize this things.
	loss function of logistic regression = log(1+e^(-y*f(x)))
	if f is some of weighted features where the weights are called beta.and I also call beta coefficient.
	
	maximum likelihood percentage = didn't understood
	
Supervised Learning Types
Regression
Classification

---------------------------------------------------------------------------------------------------------------
	Regression

	understand equations in Regression lecture in ML fundamentals.
	model = The representation of what an ML system has learned from the training data. 
	
	Interpolation = Interpolation, in mathematics, the determination or estimation of the value of f(x), or a function of x, from certain known values of the function. If x0 < … < xn and y0 = f(x0),…, yn = f(xn) are known, and if x0 < x < xn, then the estimated value of f(x) is said to be an interpolation. 
	
	Extrapolation = If x < x0 or x > xn, the estimated value of f(x) is said to be an extrapolation.
	
	Squared Loss = the squares of the difference between a model's predicted value for a labeled example and the actual value of the label. 
	
	Example = One row of a data set. An example contains one or more features and possibly a label.
	
	Mean Squared Error = The average squared loss per example. 
	
	Root Mean Squared Error = The square root of the Mean Squared Error.

	
	Arithmatic Mean = average(mathematics).
	
	weighted arithmetic mean = If each number (x) is assigned a corresponding positive weight (w), the weighted arithmetic mean is defined as the sum of their products (wx) divided by the sum of their weights.
	
	Dispersion = In statistics, dispersion (also called variability, scatter, or spread) is the extent to which a distribution is stretched or squeezed.Common examples of measures of statistical dispersion are the variance, standard deviation, and interquartile range.
	
	Standard Deviation = In statistics, the standard deviation (SD, also represented by the Greek letter sigma σ) is a measure that is used to quantify the amount of variation or dispersion of a set of data values.A low standard deviation indicates that the data points tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the data points are spread out over a wider range of values.
	
	Absolute Error = The difference between the measured or inferred value of a quantity and its actual value.
	
	Relative Error = The relative error is the absolute error divided by the magnitude of the exact value.
	
	Relative Absolute Error = The Relative absolute error is calculated as the Mean absolute error divided by the error of the ZeroR classifier (a classifier, that ignores all predictors and simply selects the most frequent value). 
	
	predictor = simple predictor is just the average of the actual values. 
	
	Relative Squared Error =  relative squared error takes the total squared error and normalizes it by dividing by the total squared error of the simple predictor.
	
	Coefficient of Determination = is the square of the correlation (r) between predicted y scores and actual y scores; thus, it ranges from 0 to 1.
	
	Correlation = is a statistical measure that indicates the extent to which two or more variables fluctuate together. A positive correlation indicates the extent to which those variables increase or decrease in parallel; a negative correlation indicates the extent to which one variable increases as the other decreases.
--------------------------------------------------------------------------------------------------------------------
	Classification
	
	classification used to predict which class or category, something belongs to.when we used model to predict values the resulting value is classed as one or zero depending upon which side of threshold line it falls.
	
	true positive = cases where model predicts 1 for test observation that actually has a value a label value of 1.
	true negative = cases where model predicts 0 for test observation that actually has a value a label value of 0.
	false positive = cases where model predicts 1 for test observation that actually has a value a label value of 0.
	false negative = cases where model predicts 0 for test observation that actually has a value a label value of 1.
	
	In case of diabetics model It might be better to have false positive, but reduce the number of false negative so that more people who are at risk gets identified.
	
	confusion matrix = A confusion matrix is a table that is often used to describe the performance of a classification model (or "classifier") on a set of test data for which the true values are known.
	
	accuracy = no of correctly classified cases to no of cases.
	
	precision = true positive test cases to total no of positive cases.(true positive + false positive)
	
	True Positive Rate/recall/sensitivity/probability of detection = true positive/(true positive + false negative)
	
	False Positive Rate/fall-out/probability of false alarm= false Positive/(false positive + true negative)
	
	Receiver Operating Characteristics(ROC) curve = The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. Area under the curve should be high.
	
------------------------------------------------------------------------------------------------------------------------
Unsupervised Learning
	clusturing = clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). 
	
	k-means clustering = 
		k = no of clusters we want to create.
		k points at random locations that represents centre point of cluster.we assign each point to closest cluster.and then we move each centroid to its true centre of points and its cluster and then we reassign each point to cluster. Repeat this process until all clusters are nicely seperated.
		
		nicely seperated means set of clusters seperate data by greatest extent possible.to measure this we can compare average distance between cluster centers to average distance between points in cluster and their center.clusters that maximizes this ratio have greatest seperation.
		we can also use ratio of average distance between clusters and max distance between points and centroid of cluster.
		
		Another way to evaluate result of clustering is Principal component Analysis(PCA).
		didn't understood PCA
		
----------------------------------------------------------------------------------------------------------------
Text-analysis
	term frequency = relative frequency of term in document = term instances/total terms
	inverse document frequency = measure of relative no of documents within which the term appears.
							   = log of total no of documents/ no of documents containing the term.
	Stemming = Stemming is a technique used to identify the words with common root and count them as same.
	porter Algorithm  = which defines sequence of rules for breaking words down in common stem based on pattern of consonants, vowels, common letter combination and word endings and other syntactical element.
	
	feature hashing = taking text that appears on multiple documents and identifying common individual words and small phrases depending upon what the NGram settings are.
	
	NGram = 
