File Reading
	read excel is too much slower than read csv so always prefer csv. If you have excel files then you can convert them into csv using following links. its faster process but requires excel to be installed in your pc.and windows.
		https://stackoverflow.com/questions/28766133/faster-way-to-read-excel-files-to-pandas-dataframe
		https://gist.github.com/tonyerskine/77250575b166bec997f33a679a0dfbe4
	Fastest format for reading and writing on disk use feather.
	
Import statement 
	It's often useful to place them inside functions to restrict their visibility and/or reduce initial startup time. 
	repeatedly executing an import statement can seriously affect performance in some circumstances.
	don't use from module import *
	which one is better "import module" or "from module import function" ?
	-> depends on scenario, importing function takes more time than importing module.but calling module.function or function directly takes nearly same time.(difference of milliseconds)
	https://stackoverflow.com/a/346967/8144864
	
String 
	string object is immutable in python. please find mutable object in python()
	use join for joining whenever possible. it is faster than regular string concatenation using plus sign.
	
List,Set,Dictionary 
	prefer use of set over list.

datetime parsing : 
	add format to increase speed of parse_date in pandas read csv.
	alternative is to use ciso8601 which is fastest datetime parser written in c language.
	Custom code in cython to improve performance
		https://stackoverflow.com/questions/14446744/speed-improvement-on-large-pandas-read-csv-with-datetime-index
	If your format input format didn't change then you can use slicing.
	You can also try datetime.date instead of datetime.datetime maybe it will be performance improvement need to check.
	numpy datetime64 should be faster than python datetime.need to check.
	if you have datetime in two columns then first join them and then convert to datetime this will improve performance.

String replacement : translate vs re.sub vs replace 
		replace will be faster than translate for small strings.
		re.sub is faste than findall and finditer for large amount of data.
		df.str.replace is faster than df.rename.
		df.str.replace didn't work with brackets.i.e. with ()
		
Regex : Some guidelines you should follow when you aren't sure what to use:
        Is the pattern you're looking for highly static? For example, do you want to split a string on every comma, pipe, or tab?
        Is resource efficiency more important than developer time? What are your priorities? Remember: Hardware is cheap, programmers are expensive.
        Are you working with HTML, XML, or other context-free grammars? Don't forget that regex has limitations.
        And my #1 rule of thumb: If you work on the problem for 5 minutes, can you rough out an idea for a non-regex approach?
	If the answer to any of these questions is "yes", you probably want string manipulation. Otherwise, consider regex.
	
Pandas Dataframe : any operation on dataframe and storing it again in same column will be slowest.
	Inplace will increase performance slightly.
	For loop with if else condition will be faster.
	list-comphrehension with dictionary will be one way to do it.
	
for loops
	apply will be better than for loop. It is used when there is no way to vectorize the given operation.
	vectorization i.e. using square brackets is also a good option.
	using numpy is the fastest way to do the operations.
	Alternatively if you wanted to use for loops you could use Cython 
	
Miscellaneous = options to reduce code runtime
	1 microservices
	2 c extensions
	3 other python runtimes such as pypy pyston grumpy etc.
	4 update cython
	
Cython 
	1 detect critical module
	2 compile it
	3 add type
	4 apply additional optimization
		a change python data structures to c data structures.
		
Python profiler 
