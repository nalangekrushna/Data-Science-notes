There are three data splitting techniques

1) Random splitting
2) Time Based Splitting (TBS)
	In this we split data based on timing. TBS works well for time series data such as food reviews which constantly changes with time. So to check on future data we could train on 10 days before today and apply it to last 10 days to check for future predictions. There are some limitations also consider a new category of food is added in last 10 days so in training data we don't have that new category.
	One caution is that check train and test data distribution should be same.
	How to determine if data is changing over time ?
	check applied ai video train and test set differences.
3) Stratified splitting


Mulit Class classification = some algo just support binary and some to multiclass classification. KNN supports multi class classification. In case of binary algo such as logistic regression it will extend to multiclass by checking if every class is present or not. by this way it will get extended to multiclass.

KNN classifier = If accuracy is same for different values of k then take largest k value as smaller k values are prone to outliers.

Local density = 

k distance = distance of k th nearest neighbour of xi from xi.
Neighbourhood of k=3 of xi = top 3 nearest neighbours of xi.
reachability distance = reachability distance between two points xi and xj is max of k th distance of xj and distance between xi and xj.
K in reachability distance = Deciding what “k” to use is very problem-specific. In the real-world, we try various values of “k” and pick the one that makes the most logical sense to the problem at hand. Please note that we do NOT have an absolute truth in outlier detection like in classification where we have a class-label to predict. Hence, we cannot apply cross-validation based methods that we typically use in classification to determine the optimal “k”.
An alternative data-driven method to determine the appropriate “k” is to use elbow method which we have described multiple times in this course as follows:
1. Plot various values of “k” on the x-axis and plot the number of outlier points as determined by LOF (which uses reachability-dist) on the y-axis.
2. At some value of “k”, the curve suddenly changes its direction. This point is called the elbow point or inflection point. For diagrams, google search: elbow method
3. The corresponding “k” where the elbow point occurs is taken as the “k”.

Local Reachability density(LRD)
LRD of xi =  inverse of average reachability distance of xi from its neighbours.
if LOF is large then it is outlier and vice versa.

procedure to find LOF
	for each point xi find its LOF(xi)
	pick points with highest LOF as outliers.
	Domain knowledge is important to decide threshhold for deciding which LOF values are outliers.
	if LOF value less than 1 means its clearly inlier but there is nothing such thing for outliers.
	LOF value could be anything in between 0 to infinity.
	
threshold for LOF = when we have to decide on the threshold, we have to use some of these methods:
1. Use manual validation and domain knowledge to determine which of the points flagged as outliers are actually outliers. We can choose our threshold now to ensure that our model mostly flags outliers only.
2. If you have lots of data and if you are ok throwing out x% of your data, you can choose a threshold that discard x% of data.
3. You can use elbow/knee method as we saw in determining “k” in k-nn to remove extreme outliers. Here, we can sort and plot LOF values on y-axis for each data-point represented om x-axis. You can now choose to use the elbow/knee point to discard outliers.

You need to use column standardization with k nearest neighbours.

interpretability =  if model gives reason why it given that answer then it is called interpretable model.
model which cannot give reasons are called black box model.KNN is interpretable if Data and k is small.