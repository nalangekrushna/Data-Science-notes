
Forward Feature Selection : first train model for every feature individually and calculate its accuracy. then add this feature and try other feature and then again check accuracy best accuracy should be selected.
We cannot select top n features of first round because may the both features will be indentical. So they may not be that much useful.

Imputation strategies :
	1) filling with mean median or mode.
	2) filling with mean median or mode using class label.
	3) filling with mean median or mode and also adding columns if the value is missing
	4) model based filling
	
Categorical variable handling :
	1) label encoding
	2) OHE
	3) filling with objective related value. such as when calculating persons heigth instead country we could take that country's avg height.
	4) using domain knowledge
	
Curse of Dimensionality
	1) machine learning = As dimensionality increase given a fixed size dataset performance of model decreases.
	2) distance function(eucludian distance) = didn't work properly. Impact is more in case of dense matrix for sparse matrix impact is some what less. Also for cosine similarity impact is lesser.
	3) Overfitting and Underfitting = As dimensionality increses chances of overfitting also increases.
	
Bias-Variance tradeoff = mathematical basis to understand underfitting and overfitting.

Generalization error = error you make on future unseen data  = square(bias) + variance + irreducible error 
	goal of ML is to have low generalization error.
	Bias = error due to simplifying assumptions.  high bias = underfitting
	variance = how much a model changes as training data changes.high variance model is overfit model.
	
Train error =  difference between predicted y and actual y on training data.if its hight then model bias is high and underfit.
Test error =  difference between predicted y and actual y on test data.if its high then model has high variance or overfit.
	

