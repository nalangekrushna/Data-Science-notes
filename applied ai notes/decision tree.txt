Entropy : for two class case entropy is maximum (i.e. 1) when both classes are 50-50.
for multi class case entropy is maximum when all classes are equi probable.
entropy can be also used for data analysis.

Information gain of a label for given feature = entropy at parent - weighted entropy of child nodes.
Gini impurity = it is same as entropy but just less scaling(i.e. max value of entropy is 1 while gini is 0.5).
and also gini impurity is computationally less expensive.

decision stump = decision tree having depth one.

runtime space and time complexity of decision tree are very low. so they can be used for low latency applications.

We use MSE(mean square error) or MAE(median absolute error) for regression in decision tree instead of information gain.

information theory

Cases
	Imbalanced data = you need to balance the data.
	large dimensionality =  for categorical feature avoid one hot encoding. instead try using probabilistic method.
	Similarity matrix = decision tree cannot work on it.
	as depth increases outliers impact will increase.
	decision tree are interpretable as they can be written as if else conditions.
	