
for training model in production environment you don't need to use all data points you could use less data points as after some amount of data later accuracy stops increasing.

for large dataset we can use ml spark which uses distributed computing and local lan network use power of all datasets.

put model in production :
	1) train model in local disk. store it in pickle format.load this pickle in production box. and predict.
	2) custom implementation = 
		a) store all parameters of model in a file.
		b) if you have low latency requirement then you can evalutate the functions in c/c++/java as this languages are much faster than python. using the parameters stored in the file.
		
for high throughput systems you can use load balancer.

Retrain model periodically.