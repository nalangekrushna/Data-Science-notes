
Before starting transformer model first look into "encoder decoder attention.txt" to get high level overview of machine translation and encoders. 

Transformer Model = a model that uses attention to boost the speed with which these models can be trained.

In encoder decoder mechanism here encoder consist of x no of 

http://jalammar.github.io/illustrated-transformer/

Imp : one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.

The Transformer model Short summary :
The Transformer model uses attention to boost training speed. the model has two components one is encoder stack and other is decoder stack. Encoder stack could contain any no of encoders but decoder stack has to same no decoder as of encoder stack. We first preprocess the text and then do word embedding on it.After that we add positional vector which model learns and able to determine the position of word in sentence. This word embedding is passed through first encoder and then output of first encoder to second and so on to the last encoder. The output of last encoder is passed to all decoders. The first decoder pops out a output which is passed to second decoder and so on until last decoder which gives us vector at current position one by one. This vector is converted using linear layer(fully connected neural network) to logit vector(one hot encoding style). on which we apply softmax function to turn vectors into probabiliites from which highest probability cell is chosen and word associated with that cell is our output.
One more things is that in each encoder and decoder after every self-attention, neural network and encoder-decoder network contains a batch normalization layer and also a residual connection skipping attention, neural and encoder-decoder.

encoder contain self attention neural network and and a feed-forward neural network. Every word in sentence pass through different position in self attention network depend on words position in sentence. Self attention layers helps us to look at other words while encoding current word.then it passes its output to neural network.

On decoder side we have three layer inside one decoder as opposed to two layers in encoder network. We have additional encoder-decoder layer in between self attention layer and neural network. encoder-decoder layer helps us to focus on relevant part of sentence. In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation. The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.

In model training step, untrained model with random initialization will go through exact same passes through encoder decoder network. but since we are training on labeled training dataset we can compare output with correct actual output.

Transformer model complete visualization
https://miro.medium.com/max/611/1*dwfspnKFLeE6o2lC0WLEMQ.png
https://towardsdatascience.com/breaking-bert-down-430461f60efb

self attention steps 
1) create a query, key, value matrix by multiplying word embedding with trained matrices.
2) Calculate score for given word with each word by multiplying query vector of given word with key vector of each other word.
3) divide score by 8.  It does score normalization.
4) take softmax of above answer. softmax decides how much weightage to be given to each word.
5) multiply softmaxed score with value vector of each word.
6) sum up all weighted value vector. This produces output of self attention layer for given word.
7) the resulting vector is sent ot neural network.
We can create multiple matrices for query, key and value to have a multi headed attention network. to get best out of it we concatenate all output after step six and then multiply it by weight matrix and then pass output to neural network.