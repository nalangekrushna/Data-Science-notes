Amazon food review

why convert text to vector
if we convert text to vector we can use all power of linear algebra there.

text-preprocessing
1) remove stop words
2) convert to lower case
3) stemming = the process of reducing inflected words to their word stem, base or root form. stemp needs not be word form.
there is one root word, but there are many variations of the same words. For example, the root word is "eat" and it's variations are "eats, eating, eaten and like so". In the same way, with the help of Stemming, we can find the root word of any variations. 
Stemming only removes suffixes and not prefixes.

Potter’s Stemmer algorithm
It is based on the idea that the suffixes in the English language are made up of a combination of smaller and simpler suffixes.
Example: EED -> EE means  as ‘agreed’ becomes ‘agree’.
To learn other stemming methods
https://www.geeksforgeeks.org/introduction-to-stemming/

4) lemmitazation = Lemmatisation in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.
It helps in returning the base or dictionary form of a word, which is known as the lemma. 
e.g. be is lemma for is are was.
For example, lemmatization would correctly identify the base form of ‘caring’ to ‘care’, whereas, stemming would cutoff the ‘ing’ part and convert it to car.
‘Caring’ -> Lemmatization -> ‘Care’
‘Caring’ -> Stemming -> ‘Car’
To learn different lametization libraries
https://www.machinelearningplus.com/nlp/lemmatization-examples-python/

5) uni-gram,bi-gram,n-gram
	in uni-gram each word is considered as dimension. In bi-gram each consecutive pairs of word is considered as dimension.
	
n-gram = an n-gram is a contiguous sequence of n words from a given sample of text or speech.
	
BoW =A popular and simple method of feature extraction with text data is called the bag-of-words model of text. A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:
	A vocabulary of known words.
	A measure of the presence of known words. 
Very simple explaination
https://victorzhou.com/blog/bag-of-words/
	
Tokenization = Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph. Tokenization didn't remove punctuations.
more about tokenization
https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer

term frequency = no of time the word occured/total no of words
Inverse document frequency = log(total no of documents/no of documents containing given word.)

formula for word value vi = Term frequency * inverse document frequency.

what tf idf does ?
	tf = more importance if word is frequent in document.
	idf = more importance to rarer word in document corpus.
	limitation = doesn't take semantic meaning.
	
Why to use log in tfidf ?
adding log is to dampen the importance of term that has a high frequency. We also add 1 to the log(tf) because when tf is equal to 1, the log(1) is zero. By adding one, we distinguish between tf=0 and tf=1.
	
word2vec = takes semantic meaning in consideration. works on dense matrix
Word2vec is a two-layer neural net that processes text. Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus. Word2vec’s applications extend beyond parsing sentences in the wild. It can be applied just as well to genes, code, likes, playlists, social media graphs and other verbal or symbolic series in which patterns may be discerned.
Word2vec represents words in vector space representation. Words are represented in the form of vectors and placement is done in such a way that similar meaning words appear together and dissimilar words are located far away. This is also termed as a semantic relationship.

sparsity of matrix = no of zeroes/total no of values.

tsne for bow,tf-idf,avg w2v,tf-idf w2v

nltk = natural language toolkit
nlp = manipulation or understanding text or speech by any software or machine.

text processing flow
remove special characters punctuation marks and html tags.
check words that are alphanumeric it should be of alphabets only.
word length should be greater that 2.
convert words to lower case.
remove stopwords.
expand contractions such as can't to can not
word stemming (porter or snowball) or lemmatization
one way of handling outlier in training data is put condition if word occurs less than some threshhold value then just remove it.
Another way is laplace smoothing.It is also applicable in case of test data.

edit distance(Levenshtein) = the minimum number of operations required to convert word1 to word2. You have the following 3 operations permitted.
1)add 2)delete 3)replace 

how to find edit distance ?
https://web.stanford.edu/class/cs124/lec/med.pdf

Hamming distance 
the Hamming distance between two strings of equal length is the number of positions at which the corresponding character are different.
Imp Note: Hamming distance assumes that length of two strings is same. so if you want to compare strings with different length then you can do padding or use edit distance. Although padding can be problematic.For example, when you compare 123 to 123456 it's different if you pad either at the end of the string or at the start of the string. The similarity of ___123 with 123456 is 0, but The similarity of 123___ with 123456 is 3.
for more explaination on this 
https://stackoverflow.com/questions/4588541/hamming-distance-vs-levenshtein-distance

String comparison methods( check below for brief.)
https://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/

The simplest way to compare two strings is with a measurement of edit distance. 
e.g. "NEW YORK METS"  "NEW YORK MEATS"
fuzz.ratio("NEW YORK METS", "NEW YORK MEATS") ⇒ 96
the standard “string closeness” measurement works fine for very short strings (such as a single word) and very long strings (such as a full book), but not so much for 3-10 word labels. The naive approach is far too sensitive to minor differences in word order, missing or extra words, and other such issues.

Partial String Similarity
fuzz.ratio("YANKEES", "NEW YORK YANKEES") ⇒ 60
fuzz.ratio("NEW YORK METS", "NEW YORK YANKEES") ⇒ 75
This doesn’t pass the intern test. The first two strings are clearly referring to the same team, but the second two are clearly referring to different ones. Yet, the score of the “bad” match is higher than the “right” one.

Inconsistent substrings are a common problem for us. To get around it, we use a heuristic we call “best partial” when two strings are of noticeably different lengths (such as the case above). If the shorter string is length m, and the longer string is length n, we’re basically interested in the score of the best matching length-m substring.
fuzz.partial_ratio("YANKEES", "NEW YORK YANKEES") ⇒ 100
fuzz.partial_ratio("NEW YORK METS", "NEW YORK YANKEES") ⇒ 69

Out Of Order
Substrings aren’t our only problem. We also have to deal with differences in string construction. Here is an extremely common pattern, where one seller constructs strings as “<HOME_TEAM> vs <AWAY_TEAM>” and another constructs strings as “<AWAY_TEAM> vs <HOME_TEAM>”
To solve this, we’ve developed two different heuristics: The “token_sort” approach and the “token_set” approach. I’ll explain both.

Token Sort : The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and then joining them back into a string.

Token Set : The token set approach is similar, but a little bit more flexible. Here, we tokenize both strings, but instead of immediately sorting and comparing, we split the tokens into two groups: intersection and remainder. We use those sets to build up a comparison string.
t0 = [SORTED_INTERSECTION]
t1 = [SORTED_INTERSECTION] + [SORTED_REST_OF_STRING1]
t2 = [SORTED_INTERSECTION] + [SORTED_REST_OF_STRING2]

t0 = "angels mariners"
t1 = "angels mariners vs"
t2 = "angels mariners anaheim angeles at los of seattle"
fuzz.ratio(t0, t1) ⇒ 90
fuzz.ratio(t0, t2) ⇒ 46
fuzz.ratio(t1, t2) ⇒ 50
fuzz.token_set_ratio("mariners vs angels", "los angeles angels of anaheim at seattle mariners") ⇒ 90
There are other ways to combine these values. For example, we could have taken an average, or a min. But in our experience, a “best match possible” approach seems to provide the best real life outcomes.


